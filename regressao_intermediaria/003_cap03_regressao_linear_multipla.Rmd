---
title: "Capítulo 3: Regressão Linear Múltipla"
author: "Edneide Ramalho"
date: "`r format(Sys.time(), '%d de %B de %Y')`"
output:
  prettydoc::html_pretty:
    theme: architect
    highlight: github
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

# Duas variáveis explicativas numéricas

```{r packages}
library(tidyverse)
library(readr)
library(broom)
library(rrcov) # fish data
```

```{r data}
# Data
data("fish")
fish <- fish

fish <- fish %>% 
  dplyr::rename("mass_g" = "Weight",
                "length_cm" = "Length1",
                "height_cm" = "Height",
                "species" = "Species")
# Nomeando as espécies
species_df <- tibble(
  species = 1:7,
  species_names = c("Bream", "Whitewish", "Roach", "Parkki",
                    "Smelt", "Pike", "Perch")
)
fish <- left_join(fish, species_df)
# Selecionando 4 espécies
selected_species <- c("Bream", "Perch", "Pike", "Roach")
fish <- fish %>% 
  dplyr::filter(species_names %in% selected_species) %>% 
  dplyr::select(mass_g, length_cm, height_cm, 
                species = species_names)
glimpse(fish)
unique(fish$species)
```

## Gráfico de Dispersão 3D

```{r}
library(plot3D)
scatter3D(fish$length_cm, fish$height_cm, fish$mass_g)
```

```{r}
library(plot3D)
library(magrittr)

fish %$%
  scatter3D(length_cm, height_cm, mass_g)
```

-   Gráficos 3D são muito difíceis de interpretar.

## Gráfico 2D com cores como resposta

```{r}
fish <- fish
ggplot(
  fish,
  aes(length_cm, height_cm, color = mass_g)
) +
  geom_point() +
  scale_color_viridis_c(option="inferno")
```

## Modelagem com 2 variáveis explicativas numéricas

```{r}
mdl_mass_vs_both <- lm(mass_g ~ length_cm + height_cm, data = fish)
mdl_mass_vs_both
```

-   No modelo há um intercepto global e uma inclinação para cada variável.

-   O fluxo de predição é o mesmo:

```{r}
explanatory_data <- expand_grid(
  length_cm = seq(5, 60, 5),
  height_cm = seq(2, 40, 2)
)

prediction_data <- explanatory_data %>% 
  dplyr::mutate(
    mass_g = predict(mdl_mass_vs_both, explanatory_data)
  )
```

-   Plot das predições:

```{r}
ggplot(fish, aes(length_cm, height_cm, color = mass_g)) +
  geom_point() +
  scale_color_viridis_c(option = "inferno") +
  geom_point(
    data = prediction_data, shape = 15, size = 3
  )
```

-   Esse gráfico nos mostra que peixes com maior comprimento e maior altura são mais pesados.

## Incluindo interação

-   Para incluir interação, basta multiplicar as variáveis:

```{r}
mdl_mass_vs_both_inter <- lm(mass_g ~ length_cm * height_cm, data = fish)
mdl_mass_vs_both_inter
```

-   Isto nos dá um termo extra de inclinação para o efeito da interação entre as duas variáveis explicativas.

-   O fluxo de predição é o mesmo:

```{r}
prediction_data_int <- explanatory_data %>% 
  mutate(
    mass_g = predict(mdl_mass_vs_both_inter, explanatory_data)
  )
```

-   Gráfico:

```{r}
ggplot(fish, aes(length_cm, height_cm, color = mass_g)) +
  geom_point() +
  scale_color_viridis_c(option = "inferno") +
  geom_point(
    data = prediction_data_int, shape = 15, size = 3
  )
```

-   O gráfico é bem parecido com o anterior, mas agora os quadrados (predição) tem cores mais próximas dos pontos (valores observados) o que indica que o modelo está bem ajustado.

## Exercícios

### Visualizações 3D

Since computer screens and paper are both two-dimensional objects, most plots are best suited to visualizing two variables at once. For the case of three continuous variables, you can draw a 3D scatter plot, but perspective problems usually make it difficult to interpret. There are some "flat" alternatives that provide easier interpretation, though they require a little thinking about to make.

`taiwan_real_estate` is available; `magrittr`, `plot3D` and `ggplot2` are loaded.

-   With the `taiwan_real_estate` dataset, draw a 3D scatter plot of the number of nearby convenience stores on the x-axis, the **square-root** of the distance to the nearest MRT stop on the y-axis, and the house price on the z-axis.

```{r}
library(fst) # to read fst format

# Data
data <-  read_fst(
  "data/taiwan_real_estate2.fst"
)

glimpse(data)
taiwan_real_estate <- data
```

```{r}
library(plot3D)
library(magrittr)

taiwan_real_estate %$%
  scatter3D(n_convenience, sqrt(dist_to_mrt_m), price_twd_msq)

```

-   With the `taiwan_real_estate` dataset, draw a scatter plot of the square-root of the distance to the nearest MRT stop versus the number of nearby convenience stores, colored by house price.

-   Use the continuous viridis color scale, using the `"plasma"` option.

```{r}
# Using taiwan_real_estate, plot sqrt dist to MRT vs. 
# no. of conv stores, colored by price
ggplot(taiwan_real_estate, aes(n_convenience, sqrt(dist_to_mrt_m), color = price_twd_msq)) + 
  # Make it a scatter plot
  geom_point() +
  # Use the continuous viridis plasma color scale
  scale_color_viridis_c(option="plasma")
```

### Modelando 2 variáveis explicativas numéricas

You already saw how to make a model and predictions with a numeric and a categorical explanatory variable. The code for modeling and predicting with two numeric explanatory variables in the same, other than a slight difference in how to specify the explanatory variables to make predictions against.

Here you'll model and predict the house prices against the number of nearby convenience stores and the square-root of the distance to the nearest MRT station.

`taiwan_real_estate` is available; `dplyr`, `tidyr` and `ggplot2` are loaded.

-   Fit a linear regression of house price versus the number of convenience stores and the square-root of the distance to the nearest MRT stations, without an interaction, using the `taiwan_real_estate` dataset.

```{r}
# Fit a linear regression of price vs. no. of conv. stores
# and sqrt dist. to nearest MRT, no interaction
mdl_price_vs_conv_dist <- lm(price_twd_msq ~ n_convenience + sqrt(dist_to_mrt_m), data = taiwan_real_estate)

# See the result
mdl_price_vs_conv_dist
```

-   Create expanded grid of explanatory variables with number of convenience stores from 0 to 10 and the distance to the nearest MRT station as a sequence from 0 to 80 in steps of 10, all squared (0, 100, 400, ..., 6400). Assign to `explanatory_data`.

-   Add a column of predictions to `explanatory_data` using `mdl_price_vs_conv_dist` and `explanatory_data`. Assign to `prediction_data`.

```{r}
explanatory_data <- expand_grid(
  n_convenience = 0:10,
  dist_to_mrt_m = (seq(0, 80, 10))^2
)

prediction_data <- explanatory_data %>% 
  dplyr::mutate(
    price_twd_msq = predict(mdl_price_vs_conv_dist, explanatory_data)
  )

prediction_data
```

-   Extend the plot to add a layer of points using the prediction data, colored yellow, with size 3.

```{r}
# Add predictions to plot
ggplot(
  taiwan_real_estate, 
  aes(n_convenience, sqrt(dist_to_mrt_m), color = price_twd_msq)
) + 
  geom_point() +
  scale_color_viridis_c(option = "plasma")+
  # Add prediction points colored yellow, size 3
  geom_point(
    data = prediction_data,
    color = "yellow", size = 3
  )
```

### Incluindo uma interação

Just as in the case with one numeric and one categorical explanatory variable, it is possible that numeric explanatory variables can interact. With this model structure, you'll get a third slope coefficient: one for each explanatory variable and one for the interaction.

Here you'll run and predict the same model as in the previous exercise, but this time including an interaction between the explanatory variables.

`taiwan_real_estate` is available; `dplyr`, `tidyr` and `ggplot2` are loaded.

-   Fit a linear regression of house price versus the number of convenience stores and the square-root of the distance to the nearest MRT stations, *with* an interaction, using the `taiwan_real_estate` dataset.

```{r}
# Fit a linear regression of price vs. no. of conv. stores
# and sqrt dist. to nearest MRT, with interaction
mdl_price_vs_conv_dist <- lm(price_twd_msq ~ n_convenience*sqrt(dist_to_mrt_m), data = taiwan_real_estate)

# See the result
mdl_price_vs_conv_dist
```

-   Create expanded grid of explanatory variables with number of convenience stores from 0 to 10 and the distance to the nearest MRT station as a sequence from 0 to 80 in steps of 10, all squared (0, 100, 400, ..., 6400). Assign to `explanatory_data`.

-   Add a column of predictions to `explanatory_data` using `mdl_price_vs_conv_dist` and `explanatory_data`. Assign to `prediction_data`.

```{r}
explanatory_data <- expand_grid(
  n_convenience = 0:10,
  dist_to_mrt_m = (seq(0, 80, 10))^2
)

prediction_data <- explanatory_data %>% 
  dplyr::mutate(
    price_twd_msq = predict(mdl_price_vs_conv_dist, explanatory_data)
  )

prediction_data
```

-   Extend the plot to add a layer of points using the prediction data, colored yellow, with size 3.

```{r}
# Add predictions to plot
ggplot(
  taiwan_real_estate, 
  aes(n_convenience, sqrt(dist_to_mrt_m), color = price_twd_msq)
) + 
  geom_point() +
  scale_color_viridis_c(option = "plasma") +
  # Add prediction points colored yellow, size 3
  geom_point(
    data = prediction_data,
    color = "yellow",
    size = 3
  )
```

# Mais de duas variáveis explicativas

## Criando facets por espécie

```{r}
ggplot(fish, aes(length_cm, height_cm, color = mass_g)) +
  geom_point() +
  scale_color_viridis_c(option = "inferno") +
  facet_wrap(vars(species))
```

## Diferentes Níveis de interação

-   Sem interações:

```{r}
lm(mass_g ~ length_cm + height_cm + species + 0, data = fish)
```

-   Interações de 2-vias:

```{r}
lm(mass_g ~ length_cm + height_cm +  species + length_cm:height_cm + length_cm:species + height_cm:species + 0,
   data = fish)
```

-   Interação de 3-vias:

```{r}
lm(mass_g ~ length_cm + height_cm +  species + length_cm:height_cm + length_cm:species + height_cm:species + length_cm:height_cm:species + 0,
   data = fish)
```

-   Quanto mais variáveis existirem, mas custoso será escrever todas as interações, mas existem atalhos, basta incluir o sinal de multiplicação `*` entre as variáveis:

```{r}
lm(mass_g ~ length_cm * height_cm * species + 0, data = fish)
```

-   Se quisermos interação de 2-vias, usamos o operador `^2`. Isso não eleva os termos ao quadrado, apenas sinaliza interações de duas vias. Para incluir termos quadráticos, adicionamos o `I` como vimos no curso anterior `lm(mass_g ~ I(length_cm)^2 + height_cm + species + 0, data = fish)`, por exemplo.

```{r}
lm(
  mass_g ~ (length_cm + height_cm + species)^2 + 0,
  data = fish
)
```

## O fluxo de predição

-   É o mesmo visto anteriormente, só que inclui mais variáveis:

```{r}
mdl_mass_vs_all <- lm(
  mass_g ~ length_cm * height_cm * species + 0,
  data = fish
)

explanatory_data <- expand_grid(
  length_cm = seq(5, 60, 6),
  height_cm = seq(2, 40, 2),
  species = unique(fish$species)
)

prediction_data <- explanatory_data %>% 
  mutate(mass_g = predict(mdl_mass_vs_all , explanatory_data))
prediction_data
```

## Visualizando as predições

```{r}
ggplot(
  fish,
  aes(length_cm, height_cm, color = mass_g)
) +
  geom_point() +
  scale_color_viridis_c(option = "inferno") +
  facet_wrap(vars(species)) +
  geom_point(
    data = prediction_data,
    size = 3, shape =15
  )

```

## Exercícios

### Visualizando muitas variáveis

As you begin to consider more variables, plotting them all at the same time becomes increasingly difficult. In addition to using x and y scales for two numeric variables, you can use color for a third numeric variable, and you can use faceting for categorical variables. And that's about your limit before the plots become to difficult to interpret. There are some specialist plot types like correlation heatmaps and parallel coordinates plots that will handle more variables, but they give you much less information about each variable, and they aren't great for visualizing model predictions.

Here you'll push the limits of the scatter plot by showing the house price, the distance to the MRT station, the number of nearby convenience stores, and the house age, all together in one plot.

`taiwan_real_estate` is available; `ggplot2` is loaded.

-   Using the `taiwan_real_estate` dataset, draw a scatter plot of `n_convenience` versus the square root of `dist_to_mrt_m`, colored by `price_twd_msq`.

-   Use the continuous viridis plasma color scale.

-   Facet the plot, wrapping by `house_age_years`.

```{r}
# Using taiwan_real_estate, no. of conv. stores vs. sqrt of
# dist. to MRT, colored by plot house price
ggplot(taiwan_real_estate, 
       aes(sqrt(dist_to_mrt_m), n_convenience, color = price_twd_msq)) +
  # Make it a scatter plot
  geom_point() +
  # Use the continuous viridis plasma color scale
   scale_color_viridis_c(option = "plasma") +
  # Facet, wrapped by house age
  facet_wrap(vars(house_age_years))
```

### Níveis diferentes de interação

Once you have three explanatory variables, the number of options for specifying interactions increases. You can specify no interactions. You can specify 2-way interactions, which gives you model coefficients for each pair of variables. The third option is to specify all the interactions, which means the three 2-way interactions and and interaction between all three explanatory variables.

As the number of explanatory variables increases further, the number of interaction possibilities rapidly increases.

-   Fit a linear regression of house price versus `n_convenience`, the square-root of `dist_to_mrt_m`, and `house_age_years`. Don't include a global intercept, and don't include any interactions.

```{r}
# Model price vs. no. of conv. stores, sqrt dist. to MRT 
# station & house age, no global intercept, no interactions
mdl_price_vs_all_no_inter <- lm(
  price_twd_msq ~ n_convenience + sqrt(dist_to_mrt_m) + 
    house_age_years + 0, data = taiwan_real_estate)


# See the result
mdl_price_vs_all_no_inter
```

-   Fit a linear regression of house price versus the square-root of `dist_to_mrt_m`, `n_convenience`, and `house_age_years`. Don't include a global intercept, but do include 2-way and 3-way interactions between the explanatory variables.

```{r}
# Model price vs. sqrt dist. to MRT station, no. of conv.
# stores & house age, no global intercept, 3-way interactions
mdl_price_vs_all_3_way_inter <- lm(
  price_twd_msq ~ sqrt(dist_to_mrt_m) * n_convenience * house_age_years + 0, 
  data = taiwan_real_estate
)

# See the result
mdl_price_vs_all_3_way_inter
```

-   Fit a linear regression of house price versus the square-root of `dist_to_mrt_m`, `n_convenience`, and `house_age_years`. Don't include a global intercept, but do include 2-way (not 3-way) interactions between the explanatory variables.

```{r}
# Model price vs. sqrt dist. to MRT station, no. of conv.
# stores & house age, no global intercept, 2-way interactions
mdl_price_vs_all_2_way_inter <- lm(
  price_twd_msq ~ (sqrt(dist_to_mrt_m) + n_convenience + house_age_years)^2 + 0, 
  data = taiwan_real_estate
)


# See the result
mdl_price_vs_all_2_way_inter
```

### Predição Novamente

You've followed the prediction workflow several times now with different combinations of explanatory variables. Time to try it once more on the model with three explanatory variables. Here, you'll use the model with 3-way interactions, though the code is the same when using any of the three models from the previous exercise.

`taiwan_real_estate` and `mdl_price_vs_all_3_way_inter` are available; `dplyr`, `tidyr` and `ggplot2` are loaded.

##### 

Make a grid of explanatory data, formed from combinations of the following variables.

-   `dist_to_mrt_m` should take a sequence from zero to eighty in steps of ten, all squared (0, 100, 400, ..., 6400).

-   `n_convenience` should take the numbers zero to ten.

-   `house_age_years` should take the unique values of the `house_age_years` column of `taiwan_real_estate`.

```{r}
explanatory_data <- expand_grid(
  dist_to_mrt_m = (seq(0, 80, 10))^2,
  n_convenience = 0:10,
  house_age_years = unique(taiwan_real_estate$house_age_years)
)

explanatory_data
```

-   Add a column to the `explanatory_data`, assigning to `prediction_data`.

-   The column should be named after the response variable, and contain predictions made using `mdl_price_vs_all_3_way_inter` and `explanatory_data`.

```{r}
prediction_data <- explanatory_data %>% 
  dplyr::mutate(
    price_twd_msq = predict(mdl_price_vs_all_3_way_inter,
                            explanatory_data)
  )
prediction_data
```

-   Extend the plot to include predictions as points from `prediction_data`, with size 3 and shape 15.

-   *Look at the plot. What do the prediction points tell you?*

```{r}
# Extend the plot
ggplot(
  taiwan_real_estate, 
  aes(sqrt(dist_to_mrt_m), n_convenience, color = price_twd_msq)
) +
  geom_point() +
  scale_color_viridis_c(option = "plasma") +
  facet_wrap(vars(house_age_years)) +
  # Add points from prediction data, size 3, shape 15
  geom_point(data = prediction_data,
             size = 3, shape = 15)
```

Os gráficos mostram que quanto maior a raiz quadrada da distância ao MRT mais próximoe menor o número de lojas de conveniência, mais baratas são as casas. Isto é maior para casas com menos de 15 anos.

# Como a regressão linear funciona

-   Soma dos quadrados

-   Queremos encontrar o intercepto e a inclinação que minimiza a soma dos quadrados dos resíduos.

## `optm()`

```{r}
calc_quadratic <- function(x){
  x^2 - x + 10
}
```

```{r}
optim(par = 3, fn = calc_quadratic)
```

-   par é o chute inicial

## Refinamento

```{r}
calc_quadratic <- function(coeffs)[
  x <- coeffs[1]
  x^2 - x + 10
]
```

```{r}
optim(par = c(x=3), fn = calc_quadratic)
```

## Exercícios

### A soma dos quadrados

In order to choose the "best" line to fit the data, regression models need to optimize some metric. For linear regression, this metric is called the *sum of squares*.

In the dashboard, try setting different values of the intercept and slope coefficients. In the plot, the solid black line has the intercept and slope you specified. The dotted blue line has the intercept and slope calculated by a linear regression on the dataset.

How does linear regression try to optimize the sum of squares metric?

-   Linear regression makes the sum of the squares of the differences between the actual responses and the predicted responses zero.

-   Linear regression makes the sum of the squares of the differences between the actual responses and the predicted responses infinite.

-   Linear regression maximizes the sum of the squares of the differences between the actual responses and the predicted responses.

-   Linear regression minimizes the sum of the squares of the differences between the actual responses and the predicted responses. CORRECT!!!!

### Algoritmo de regressão linear

To truly understand linear regression, it is helpful to know how the algorithm works. The code for `lm()` is hundreds of lines because it has to work with any formula and any dataset. However, in the case of simple linear regression for a single dataset, you can implement a linear regression algorithm in just a few lines of code.

The workflow is

1.  Write a script to calculate the sum of squares.

2.  Turn this into a function.

3.  Use R's general purpose optimization function find the coefficients that minimize this.

The explanatory values (the `n_convenience` column of `taiwan_real_estate`) are available as `x_actual`. The response values (the `price_twd_msq` column of `taiwan_real_estate`) are available as `y_actual`.

-   Set the intercept to ten.

-   Set the slope to one.

-   Calculate the predicted y-values as the intercept plus the slope times the actual x-values.

-   Calculate the differences between actual and predicted y-values.

-   Calculate the sum of squares. Get the sum of the differences in y-values, squaring each value.

```{r, eval=FALSE}
# Set the intercept to 10
intercept <- 10

# Set the slope to 1
slope <- 1

# Calculate the predicted y values
y_pred <- intercept + slope * x_actual

# Calculate the differences between actual and predicted
y_diff <- y_actual - y_pred

# Calculate the sum of squares
sum(y_diff^2) 
```

Complete the function body.

-   Get the intercept from the first element of `coeffs`.

-   Get the slope from the second element of `coeffs`.

-   Calculate the predicted y-values as the intercept plus the slope times the actual x-values.

-   Calculate the differences between actual and predicted y-values.

-   Calculate the sum of squares. Get the sum of the differences in y-values, squaring each value.

```{r}
calc_sum_of_squares <- function(coeffs) {
  # Get the intercept coeff
  intercept <- coeffs[1]

  # Get the slope coeff
  slope <- coeffs[2]

  # Calculate the predicted y values
  y_pred <- intercept + slope * x_actual

  # Calculate the differences between actual and predicted
  y_diff <- y_actual - y_pred

  # Calculate the sum of squares
  sum(y_diff^2) 
}
```

Optimize the sum of squares metric.

-   Call an optimization function.

-   Initially guess that the intercept is zero and the slope is zero by passing a named vector of parameters.

-   Use `calc_sum_of_squares` as the optimization function.

```{r, eval=FALSE}
# From previous step
calc_sum_of_squares <- function(coeffs) {
  intercept <- coeffs[1]
  slope <- coeffs[2]
  y_pred <- intercept + slope * x_actual
  y_diff <- y_actual - y_pred
  sum(y_diff ^ 2)
}

# Optimize the metric
optim(
  # Initially guess 0 intercept and 0 slope
  par = c(intercept = 0, slope = 0), 
  # Use calc_sum_of_squares as the optimization fn
  fn = calc_sum_of_squares
)

# Compare the coefficients to those calculated by lm()
lm(price_twd_msq ~ n_convenience, data = taiwan_real_estate)
```
