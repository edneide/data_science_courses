---
title: "Regressão Logística Múltipla"
author: "Edneide Ramalho"
date: "`r format(Sys.time(), '%d de %B de %Y')`"
output: 
    html_document:
      highlight: textmate
      logo: logo.png
      theme: cerulean
      number_sections: yes
      toc: yes
      toc_float:
        collapsed: yes
        smooth_scroll: no
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

# Pacotes

```{r}
library(tidyverse)
library(yardstick)
```

# Data

```{r data}
library(fst) # to read fst format

# Data
data <-  read_fst(
  "data/churn.fst"
)

glimpse(data)
churn <- data
```

# `glm()`

-   Criando um modelo:

```{r, eval=FALSE}
# Logistic regression
glm(response ~ explanatory, data = dataset, family = binomial)
```

-   Para múltiplas variáveis explicativas:

```{r, eval = FALSE}
glm(response ~ explanatory1 + explanatory2, 
    data = dataset, family = binomial)
```

-   Com interações entre variáveis:

```{r, eval = FALSE}
glm(response ~ explanatory1 * explanatory2, 
    data = dataset, family = binomial)
```

# Exercícios

## **Visualizing multiple explanatory variables**

Logistic regression also supports multiple explanatory variables. Plotting has similar issues as the linear regression case: it quickly becomes difficult to include more numeric variables in the plot. Here we'll look at the case of two numeric explanatory variables, and the solution is basically the same as before: use color to denote the response.

Here there are only two possible values of response (zero and one), and later when we add predicted responses, the values all lie between zero and one. Once you include predicted responses, the most important thing to determine from the plot is whether the predictions are close to zero, or close to one. That means that a 2-color gradient split at 0.5 is really useful: responses above 0.5 are one color, and responses below 0.5 are another color.

The bank churn dataset is available as `churn`; `ggplot2` is loaded.

-   Using the `churn` dataset, plot the recency of purchase, `time_since_last_purchase`, versus the length of customer relationship, `time_since_first_purchase`, colored by whether or not the customer churned, `has_churned`.

-   Add a point layer, with transparency set to `0.5`.

-   Use a 2-color gradient, with midpoint `0.5`.

-   Use the black and white theme.

```{r}
# Using churn, plot recency vs. length of relationship,
# colored by churn status
ggplot(churn, aes(time_since_first_purchase, time_since_last_purchase, color = has_churned)) +
  # Make it a scatter plot, with transparency 0.5
  geom_point(alpha = 0.5) +
  # Use a 2-color gradient split at 0.5
  scale_color_gradient2(midpoint = 0.5) +
  # Use the black and white theme
  theme_bw()
```

## **Logistic regression with 2 explanatory variables**

To include multiple explanatory variables in logistic regression models, the syntax is the same as for linear regressions. The only change is the same as in the simple case: you run a *generalized* linear model with a binomial error family.

Here you'll fit a model of churn status with both of the explanatory variables from the dataset: the length of customer relationship and the recency of purchase.

`churn` is available.

-   Fit a logistic regression of churn status, `has_churned` versus length of customer relationship, `time_since_first_purchase` and recency of purchase, `time_since_last_purchase`, and an interaction between the explanatory variables.

```{r}
# Fit a logistic regression of churn status vs. length of relationship, recency, and an interaction
mdl_churn_vs_both_inter <- glm(has_churned ~ time_since_first_purchase * time_since_last_purchase,
                               data = churn,
                               family = "binomial")
 
# See the result
mdl_churn_vs_both_inter
```

## **Logistic regression prediction**

As with linear regression, the joy of logistic regression is that you can make predictions. Let's step through the prediction flow one more time!

`churn` and `mdl_churn_vs_both_inter` are available; `dplyr`, `tidyr` and `ggplot2` are loaded.

Create a grid of explanatory variables.

-   Set `time_since_first_purchase` to a sequence from minus two to four in steps of `0.1`.

-   Set `time_since_last_purchase` to a sequence from minus one to six in steps of `0.1`.

```{r}
explanatory_data <- expand_grid(
  time_since_first_purchase = seq(-2, 4, by = 0.1),
time_since_last_purchase = seq(-1, 6, by = 0.1)
)

head(explanatory_data)
```

-   Add a column to `explanatory_data` named `has_churned` containing predictions using `mdl_churn_vs_both_inter` and `explanatory_data` with type `"response"`.

```{r}
# Add a column of predictions using mdl_churn_vs_both_inter and explanatory_data with type response
prediction_data <- explanatory_data %>% 
  dplyr::mutate(
    has_churned = predict(mdl_churn_vs_both_inter, explanatory_data, type = "response")
  )

# See the result
prediction_data
```

-   Extend the plot by adding points from `prediction_data` with size 3 and shape 15.

```{r}
# Extend the plot
ggplot(
  churn, 
  aes(time_since_first_purchase, time_since_last_purchase, color = has_churned)
) +
  geom_point(alpha = 0.5) +
  scale_color_gradient2(midpoint = 0.5) +
  theme_bw() +
  # Add points from prediction_data with size 3 and shape 15
  geom_point(data = prediction_data, size = 3, shape = 15)
```

## **Confusion matrix**

When the response variable has just two outcomes, like the case of churn, the measures of success for the model are "how many cases where the customer churned did the model correctly predict?" and "how many cases where the customer didn't churn did the model correctly predict?". These can be found by generating a confusion matrix and calculating summary metrics on it. A mosaic plot is the natural way to visualize the results.

`churn` and `mdl_churn_vs_both_inter` are available; `dplyr` and `yardstick` are loaded.

-   Get the actual responses from the `churn` dataset.

-   Get the predicted responses from the rounded, fitted values of `mdl_churn_vs_both_inter`.

-   Create a table of the actual and predicted response values.

-   Convert the table to a `conf_mat` confusion matrix object.

```{r}
# Get the actual responses from churn
actual_response <- churn$has_churned

# Get the predicted responses from the model
predicted_response <- round(fitted(mdl_churn_vs_both_inter))

# Get a table of these values
outcomes <- table(predicted_response, actual_response)

# Convert the table to a conf_mat object
confusion <- conf_mat(outcomes)

# See the result
confusion
```

-   "Automatically" plot the confusion matrix, `confusion`.

-   Get summary metrics from the confusion matrix. Remember that the churn event is in the second row/column of the matrix.

```{r}
autoplot(confusion)
```

```{r}
summary(confusion, event_level = "second")
```

# A distribuição logística

Antes, vamos ver a distribuição Gaussiana.

```{r}
gaussian_distn <- tibble(
  x = seq(-4, 4, 0.05),
  gauss_pdf_x = dnorm(x)
)
```

```{r}
ggplot(gaussian_distn,
       aes(x, gauss_pdf_x)) +
  geom_line()
```

## Distribuição Gaussiana acumulada

```{r}
gaussian_distn <- tibble(
  x = seq(-4, 4, 0.05),
  gauss_pdf_x = dnorm(x),
  gauss_cdf_x = pnorm(x)
)

ggplot(gaussian_distn,
       aes(x, gauss_cdf_x)) +
  geom_line()
```

## Inverso da distribuição Gaussiana acumulada

```{r}
gaussian_distn_inv <- tibble(
  p = seq(0.001, 0.999, 0.001),
  gauss_inv_cdf_p = qnorm(p)
)
```

```{r}
ggplot(gaussian_distn_inv,
       aes(p, gauss_inv_cdf_p)) +
  geom_line()
```

-   `linkfun` é a transformação de variável repsosta.

```{r}
gaussian()$linkfun
```

```{r}
gaussian()$linkinv
```

# Fdp Logística (função distribuição de probabilidade)

```{r}
logistic_distn <- tibble(
  x = seq(-6, 6, 0.05),
  logistic_pdf_x = dlogis(x)
)
```

```{r}
ggplot(logistic_distn,
       aes(x, logistic_pdf_x)) +
  geom_line()
```

$$
cdf(x) = \dfrac{1}{(1 - exp(-x))}
$$

Eq. da distribuição logística.

-   A inversa é chamada de função **logit** e é dada por:

$$
\text{inverse_cdf(p)} = log\left(\dfrac{1}{1-p}\right)
$$

# Exercícios

## Função de distribuição acumulada

Understanding the logistic distribution is key to understanding logistic regression. Like the normal (Gaussian) distribution, it is a probability distribution of a single continuous variable. Here you'll visualize the *cumulative distribution function* (CDF) for the logistic distribution. That is, if you have a logistically distributed variable, `x`, and a possible value, `xval`, that `x` could take, then the CDF gives the probability that `x` is less than `xval`.

The logistic distribution's CDF is calculated with the logistic function (hence the name). The plot of this has an S-shape, known as a *sigmoid curve*. An important property of this function is that it takes an input that can be any number from minus infinity to infinity, and returns a value between zero and one.

Create a tibble containing three columns.

-   `x` values as a sequence from minus ten to ten in steps of `0.1`.

-   `logistic_x` made from `x` transformed with the logistic distribution CDF.

-   `logistic_x_man` made from `x` transformed with a logistic function calculated from the equation cdf(x)=1/(1+exp(−x)).

-   Check that both logistic transformations (`logistic_x` and `logistic_x_man`) have the same values with `all.equal()`.

```{r}
logistic_distn_cdf <- tibble(
  # Make a seq from -10 to 10 in steps of 0.1
  x = seq(-10, 10, by = 0.1),
  # Transform x with built-in logistic CDF
  logistic_x = plogis(x),
  # Transform x with manual logistic
  logistic_x_man = 1/(1 + exp(-x))
) 

# Check that each logistic function gives the same results
all.equal(
  logistic_distn_cdf$logistic_x, 
  logistic_distn_cdf$logistic_x_man
)
```

-   Using the `logistic_distn_cdf` dataset, plot `logistic_x` versus `x` as a line plot.

```{r}
# Using logistic_distn_cdf, plot logistic_x vs. x
ggplot(logistic_distn_cdf, aes(x, logistic_x)) +
  # Make it a line plot
  geom_line()
```

## **Inverse cumulative distribution function**

The logistic function (logistic distribution CDF) has another important property: each x input value is transformed to a unique value. That means that the transformation can be reversed. The *logit function* is the name for the *inverse logistic function*, which is also the *logistic distribution inverse cumulative distribution function*. (All three terms mean exactly the same thing.)

The logit function takes values between zero and one, and returns values between minus infinity and infinity.

Create a tibble containing three columns.

-   `x` values as a sequence from minus `0.001` to `0.999` in steps of `0.001`.

-   `logit_p` made from `p` transformed with the logistic distribution inverse CDF.

-   `logit_p_man` made from `p` transformed with the equation log(p(1−p)).

-   Check that both logit transformations (`logit_p` and `logit_p_man`) have the same values with `all.equal()`.

```{r}
logistic_distn_inv_cdf <- tibble(
  # Make a seq from 0.001 to 0.999 in steps of 0.001
  p = seq(0.001, 0.999, by = 0.001),
  # Transform with built-in logistic inverse CDF
  logit_p = qlogis(p),
  # Transform with manual logit
  logit_p_man = log(p/(1-p))
) 

# Check that each logistic function gives the same results
all.equal(
  logistic_distn_inv_cdf$logit_p,
  logistic_distn_inv_cdf$logit_p_man
)
```

-   Using the `logistic_distn_inv_cdf` dataset, plot `logit_p` versus `p` as a line plot.

```{r}
ggplot(logistic_distn_inv_cdf, aes(p, logit_p)) +
  geom_line()
```

## **binomial family argument**

The big difference between running a linear regression with `lm()` and running a logistic regression with `glm()` is that you have to set `glm()`'s `family` argument to `binomial`. `binomial()` is a function that returns a list of other functions that tell `glm()` how to perform calculations in the regression. The two most interesting functions are `linkinv` and `linkfun`, which are used for transforming variables from the whole number line (minus infinity to infinity) to probabilities (zero to one) and back again.

A vector of values, `x`, and a vector of probabilities, `p`, are available.

-   Examine the structure of the `binomial()` function. *Notice that it contains two elements that are functions,* `binomial()$linkinv`*, and* `binomial()$linkfun`.

-   Call `binomial()$linkinv()` on `x`, assigning to `linkinv_x`.

-   Check that `linkinv_x` and `plogis()` of `x` give the same results with `all.equal()`.

-   Call `binomial()$linkfun()` on `p`, assigning to `linkfun_p`.

-   Check that `linkfun_p` and `qlogis()` of `p` give the same results.

```{r}
x <- seq(-10, 10, by = 0.2)
p <- seq(0.01, 0.99, by = 0.01)
```

```{r}
str(binomial())
```

```{r}
# Call the link inverse on x
linkinv_x <- binomial()$linkinv(x)

# Check linkinv_x and plogis() of x give same results 
all.equal(
  linkinv_x, plogis(x)
)

# Call the link fun on p
linkfun_p <- binomial()$linkfun(p)

# Check linkfun_p and qlogis() of p give same results  
all.equal(
  linkfun_p,
  qlogis(p)
)
```

## **Logistic distribution parameters**

The logistic distribution CDF is not just a single curve. In the same way that the normal distribution has mean and standard deviation parameters that affect the CDF curve, the logistic distribution has *location* and *scale* parameters. Here, you'll visualize how changing those parameters changes the CDF curve.

How do changes to the parameters change the CDF curve?

-   As \`location\` increases, the logistic CDF curve moves rightwards. As \`scale\` increases, the steepness of the slope increases.

-   As \`location\` increases, the logistic CDF curve moves upwards. As \`scale\` increases, the steepness of the slope increases.

-   As \`location\` increases, the logistic CDF curve moves rightwards. As \`scale\` increases, the steepness of the slope decreases. **CORRECT!!!!**

-   As \`location\` increases, the logistic CDF curve moves upwards. As \`scale\` increases, the steepness of the slope decreases.

# Como a regressão logística funciona

-   Soma dos quadrados não funciona na regressão logística.

-   Predições e observações estão sempre entre 0 e 1.

-   Há uma métrica melhor que a soma dos quadrados.

-   Likelihood: `sum(y_pred * y_actual + (1 - y_pred) * (1 - y_actual))`

-   Quando `y_actual = 1`:

    -   `y_pred * 1 + (1 - y_pred) * (1 - 1) = y_pred`

    -   Para aumentar a máxima verossimilhança y_pred deve ser o máximo, ou seja, igual a 1.

-   Quando `y_actual=0`

    -   `y_pred * 0 + (1 - y_pred) * (1 - 0) = 1 - y_pred`

    -   A máxima verossimilhança ocorre quando y_pred é zero.

## Log-likelihood

É mais fácil de computar e não somamos valores tão próximos de zero.

`log(y_pred) * y_actual + log(1 - y_pred) * (1-y_actual)`

## Log negativo da verossimilhança

-   Maximizar o log da verossimilhança é o mesmo que minimizar o log negativo da verossimilhança:

`-sum(log_likelihoods)`

# Exercícios

#### Likelihood & log-likelihood

Linear regression tries to optimize a "sum of squares" metric in order to find the best fit. That metric isn't applicable to logistic regression. Instead, logistic regression tries to optimize a metric called *likelihood*, or a related metric called *log-likelihood*.

The dashboard shows churn status versus line time since last purchase from the `churn` dataset. The blue dotted line is the logistic regression prediction line calculated by ggplot2's `geom_smooth()`. (That is, it's the "best fit" line.) The black solid line shows a prediction line calculated from the intercept and slope coefficients you specify as `plogis(intercept + slope * time_since_last_purchase)`.

Change the intercept and slope coefficients and watch how the likelihood and log-likelihood values change.

As you get closer to the best fit line, what statement is true about likelihood and log-likelihood?

##### Instructions

**50XP**

#### 

-   Both likelihood and log-likelihood increase to a maximum value.

-   Both likelihood and log-likelihood decrease to a minimum value.

-   Likelihood increases to a maximum value; log-likelihood decreases to a minimum value.

-   Likelihood decreases to a minimum value; log-likelihood increases to a maximum value.

    #### **Likelihood & log-likelihood**

    Linear regression tries to optimize a "sum of squares" metric in order to find the best fit. That metric isn't applicable to logistic regression. Instead, logistic regression tries to optimize a metric called *likelihood*, or a related metric called *log-likelihood*.

    The dashboard shows churn status versus line time since last purchase from the `churn` dataset. The blue dotted line is the logistic regression prediction line calculated by ggplot2's `geom_smooth()`. (That is, it's the "best fit" line.) The black solid line shows a prediction line calculated from the intercept and slope coefficients you specify as `plogis(intercept + slope * time_since_last_purchase)`.

    Change the intercept and slope coefficients and watch how the likelihood and log-likelihood values change.

    As you get closer to the best fit line, what statement is true about likelihood and log-likelihood?

    -   Both likelihood and log-likelihood increase to a maximum value. **CORRECT!!!!**

    -   Both likelihood and log-likelihood decrease to a minimum value.

    -   Likelihood increases to a maximum value; log-likelihood decreases to a minimum value.

    -   Likelihood decreases to a minimum value; log-likelihood increases to a maximum value.

## **Logistic regression algorithm**

Let's dig into the internals and implement a logistic regression algorithm. Since R's `glm()` function is very complex, you'll stick to implementing simple logistic regression for a single dataset.

Rather than using sum of squares as the metric, we want to use likelihood. However, log-likelihood is more computationally stable, so we'll use that instead. Actually, there is one more change: since we want to maximize log-likelihood, but `optim()` defaults to finding minimum values, it is easier to calculate the *negative* log-likelihood.

The log-likelihood value for each observation is

`log(ypred)∗yactual+log(1−ypred)∗(1−yactual)`

The metric to calculate is minus the sum of these log-likelihood contributions.

The explanatory values (the `time_since_last_purchase` column of `churn`) are available as `x_actual`. The response values (the `has_churned` column of `churn`) are available as `y_actual`.

```{r}
x_actual <- churn$time_since_last_purchase
y_actual <-  churn$has_churned
```

-    Set the intercept to one.

-   Set the slope to `0.5`.

-   Calculate the predicted y-values as the intercept plus the slope times the actual x-values, all transformed with the logistic distribution CDF.

-   Calculate the log-likelihood for each term as the log of the predicted y-values times the actual y-values, plus the log of one minus the predicted y-values times one minus the actual y-values.

-   Calculate minus the sum of the log-likelihoods for each term.

```{r}
# Set the intercept to 1
intercept <- 1

# Set the slope to 0.5
slope <- 0.5

# Calculate the predicted y values
y_pred <- plogis(intercept + slope * x_actual)

# Calculate the log-likelihood for each term
log_likelihoods <- log(y_pred) * y_actual + log(1 - y_pred)*(1-y_actual)

# Calculate minus the sum of the log-likelihoods for each term
-sum(log_likelihoods)
```

Complete the function body.

-   Get the intercept from the first element of `coeffs`.

-   Get the slope from the second element of `coeffs`.

-   Calculate the predicted y-values as the intercept plus the slope times the actual x-values, transformed with the logistic distribution CDF.

-   Calculate the log-likelihood for each term as the log of the predicted y-values times the actual y-values, plus the log of one minus the predicted y-values times one minus the actual y-values.

-   Calculate minus the sum of the log-likelihoods for each term.

```{r}
calc_neg_log_likelihood <- function(coeffs) {
  # Get the intercept coeff
  intercept <- coeffs[1]

  # Get the slope coeff
  slope <- coeffs[2]

  # Calculate the predicted y values
  y_pred <- plogis(intercept + slope * x_actual)

  # Calculate the log-likelihood for each term
  log_likelihoods <- log(y_pred) * y_actual + log(1 - y_pred)*(1-y_actual)

  # Calculate minus the sum of the log-likelihoods for each term
  -sum(log_likelihoods)
}
```

Optimize the sum of squares metric.

-   Call an optimization function.

-   Initially guess that the intercept is zero and the slope is one.

-   Use `calc_neg_log_likelihood` as the optimization function.

```{r}
# Optimize the metric
optim(
  # Initially guess 0 intercept and 1 slope
  par = c(intercept = 0, slope = 1),
  # Use calc_neg_log_likelihood as the optimization fn 
  fn = calc_neg_log_likelihood
)

# Compare the coefficients to those calculated by glm()
glm(has_churned ~ time_since_last_purchase, data = churn, family = binomial)
```
